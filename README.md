# nanoMolmo2

**ğŸ“ A Vision-Language Model (VLM) Learning Project**

A minimal implementation of Molmo2 VLM from scratch for educational purposes.

## Overview

**nanoMolmo2** is an educational reimplementation of the [Molmo2](https://molmo.allenai.org/) Vision-Language Model, designed to help developers **learn and understand modern VLM architectures from the ground up**. This hands-on project uses **Qwen3-0.6B** as the base language model while following Molmo2's architecture and training methodology.

ğŸ¯ **Primary Goal**: Provide a clear, educational implementation for learning how Vision-Language Models work - from architecture design to multimodal training.

> âš ï¸ **Note**: This is a **learning-focused educational project**, not intended for production use.

## Architecture

**nanoMolmo2**: Educational VLM with frozen vision encoder for efficiency

- **Vision Encoder**: Molmo2's CLIP ViT (~300M params) - **ğŸ”’ FROZEN during training**
- **Connector**: Linear/MLP projection (~1M params) - **âœï¸ TRAINABLE**
- **Base LLM**: Qwen3-0.6B (~500M params) - **âœï¸ TRAINABLE**
- **Training Objective**: Same as Molmo2 (multimodal next-token prediction)

**Why frozen vision encoder?**
- âœ… **50% less memory** (~20GB vs ~30GB per GPU)
- âœ… **30-40% faster training** (skip vision backward pass)
- âœ… **Stable features** (pre-trained CLIP is already excellent)
- âœ… **Focus learning** on language understanding

**Total trainable**: ~501M parameters  
**Hardware**: Runs on 2-4 A100 40GB GPUs

See [MODEL_ARCHITECTURE.md](./MODEL_ARCHITECTURE.md) for complete details.

## Quick Start

### Step 1: Install dependencies

```bash
pip install -r requirements.txt
```

### Step 2: Download datasets

```bash
# Stage 1 pre-training only (~80GB, 5 datasets)
python scripts/download_datasets.py --stage pretrain

# Stage 2 & 3 SFT datasets (~500GB, 29 datasets)
python scripts/download_datasets.py --stage sft

# All stages
python scripts/download_datasets.py --stage all

# Useful options:
python scripts/download_datasets.py --list      # See all datasets
python scripts/download_datasets.py --check     # Check download status
python scripts/download_datasets.py --dry-run   # Preview without downloading
```

**Storage by Stage** (based on Molmo2 tech report):
- **Stage 1**: ~80GB (5 datasets) - Pre-training with fixed ratios
- **Stage 2 & 3**: ~500GB (29 datasets) - SFT (Stage 3 uses same data)

See [DATASETS_BY_STAGE.md](./DATASETS_BY_STAGE.md) for complete breakdown.

### Step 3: Train

```bash
# Stage 1: Pre-training
python examples/train_with_stage_dataloaders.py --stage 1

# Stage 2: SFT
python examples/train_with_stage_dataloaders.py --stage 2

# Stage 3: Long-context (same data, longer sequences)
python examples/train_with_stage_dataloaders.py --stage 3
```

## Project Structure

```
nanoMolmo2/
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ model_config.yaml         # Model architecture config
â”‚   â””â”€â”€ train_config.yaml         # Training parameters
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ dataloaders/              # Dataset implementations
â”‚   â”‚   â”œâ”€â”€ base.py               # Base classes
â”‚   â”‚   â”œâ”€â”€ image_datasets.py     # Image dataset loaders
â”‚   â”‚   â”œâ”€â”€ video_datasets.py     # Video dataset loaders
â”‚   â”‚   â””â”€â”€ utils.py              # Utilities (packing, weighting)
â”‚   â””â”€â”€ stage_dataloaders.py      # Stage-specific data modules
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ minimal_pure_pytorch.py   # Minimal VLM implementation
â”‚   â””â”€â”€ train_with_stage_dataloaders.py  # Training example
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ download_datasets.py      # Dataset downloader
â”‚   â”œâ”€â”€ inspect_molmo2_data.py    # Data inspection tool
â”‚   â””â”€â”€ verify_model_setup.py     # Setup verification
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_dataloaders.py       # Unit tests
â”œâ”€â”€ DATASETS_BY_STAGE.md          # Dataset breakdown by stage
â”œâ”€â”€ MODEL_ARCHITECTURE.md         # Architecture details
â”œâ”€â”€ MOLMO2_TECH_REPORT_SUMMARY.md # Tech report summary
â”œâ”€â”€ PURE_PYTORCH_GUIDE.md         # Pure PyTorch implementation
â”œâ”€â”€ QUICKSTART.md                 # Quick start guide
â”œâ”€â”€ TRAINING_PIPELINE.md          # Training pipeline details
â”œâ”€â”€ YOUR_SETUP.md                 # Your specific setup
â”œâ”€â”€ requirements.txt              # Full dependencies
â””â”€â”€ requirements_minimal.txt      # Minimal dependencies
```

## Training Pipeline

Based on Molmo2's 3-stage approach:

```
Stage 1: Pre-training (5 datasets, ~80GB)
â”œâ”€â”€ 60% Dense captioning (PixMo-Cap)
â”œâ”€â”€ 30% Image pointing (PixMo-Points, PixMo-Count, CoSyn-Point)
â””â”€â”€ 10% NLP data (Tulu)
    â†“
Stage 2: Supervised Fine-Tuning (100+ datasets)
â”œâ”€â”€ Molmo2 datasets (video cap, QA, pointing, tracking)
â”œâ”€â”€ PixMo datasets (image cap, QA, pointing)
â””â”€â”€ Academic datasets (VQA, DocVQA, ChartQA, ...)
    â†“
Stage 3: Long-Context SFT (same datasets as Stage 2)
â”œâ”€â”€ Longer sequences: 36,864 tokens (vs 4,096)
â””â”€â”€ More frames: 384 (vs 128)
```

## Documentation

- **[DATASETS_BY_STAGE.md](./DATASETS_BY_STAGE.md)** - Complete dataset breakdown
- **[MODEL_ARCHITECTURE.md](./MODEL_ARCHITECTURE.md)** - Architecture details
- **[MOLMO2_TECH_REPORT_SUMMARY.md](./MOLMO2_TECH_REPORT_SUMMARY.md)** - Molmo2 paper summary
- **[TRAINING_PIPELINE.md](./TRAINING_PIPELINE.md)** - Training pipeline guide
- **[PURE_PYTORCH_GUIDE.md](./PURE_PYTORCH_GUIDE.md)** - Pure PyTorch implementation
- **[QUICKSTART.md](./QUICKSTART.md)** - Detailed quick start
- **[YOUR_SETUP.md](./YOUR_SETUP.md)** - Your specific configuration

## References

- [Molmo2 Blog Post](https://allenai.org/blog/molmo2)
- [Molmo2 Technical Report](https://molmo.allenai.org/)
- [Qwen3 Model](https://github.com/QwenLM/Qwen3)
- [HuggingFace Molmo2 Collection](https://huggingface.co/collections/allenai/molmo2-data)

## License

MIT License - See LICENSE file for details.

---

**Educational Use Only** | Built with â¤ï¸ for learning VLM architectures
