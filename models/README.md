# NanoMolmo2 Model Components

Pure PyTorch implementation of a Vision-Language Model (VLM) based on the Molmo2 architecture.

## Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         NanoMolmo2 Architecture                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

   Input Image (384Ã—384)
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Vision Encoder        â”‚  SigLIP 2 So400m/14 @ 384px
â”‚    (413M params)         â”‚  
â”‚    ğŸ”’ FROZEN             â”‚  Output: [B, 729, 1152]
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  (729 patches Ã— 1152 dim)
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Connector             â”‚  Linear/MLP/Resampler
â”‚    (~1-4M params)        â”‚  
â”‚    âœï¸  TRAINABLE         â”‚  Output: [B, 729, 1024]
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  (729 tokens Ã— LLM dim)
          â”‚
          â–¼
   Visual Tokens â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                 â”‚
   Text Tokens â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â–¶ Concatenated Sequence
          â”‚                      â”‚
          â–¼                      â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚    Token Embedding       â”‚     â”‚
â”‚    (Qwen3 vocab)         â”‚     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
          â”‚                      â”‚
          â–¼                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Language Model (LLM Decoder)             â”‚
â”‚              Qwen3-0.6B-Base                          â”‚
â”‚              (596M params, 28 layers)                 â”‚
â”‚              âœï¸  TRAINABLE                            â”‚
â”‚                                                       â”‚
â”‚  Features:                                            â”‚
â”‚  â€¢ Grouped Query Attention (16 Q heads, 8 KV heads)  â”‚
â”‚  â€¢ Rotary Position Embeddings (RoPE)                  â”‚
â”‚  â€¢ Q/K Normalization (Qwen3-specific)                 â”‚
â”‚  â€¢ SwiGLU MLP activation                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
   Output Logits [B, seq_len, 151936]
```

---

## Components

### 1. Vision Encoder (`vision_encoder.py`)

**Model**: SigLIP 2 So400m/14 @ 384px  
**Parameters**: 413M (frozen during training)  
**Source**: [google/siglip2-so400m-patch14-384](https://huggingface.co/google/siglip2-so400m-patch14-384)

#### Architecture

| Component | Value |
|-----------|-------|
| Image Size | 384 Ã— 384 |
| Patch Size | 14 Ã— 14 |
| Num Patches | 729 (27 Ã— 27) |
| Hidden Dim | 1152 |
| MLP Dim | 4304 |
| Layers | 27 |
| Attention Heads | 16 |

#### Data Flow

```
Input: [B, 3, 384, 384] RGB image
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Patch Embedding    â”‚  Conv2d(3, 1152, kernel=14, stride=14)
â”‚  + Position Embed   â”‚  Learnable position embeddings
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼  [B, 729, 1152]
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Transformer Layers â”‚  27 Ã— EncoderLayer
â”‚  (Pre-Norm)         â”‚  LayerNorm â†’ Attention â†’ LayerNorm â†’ MLP
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼  [B, 729, 1152]
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Post LayerNorm     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
Output: [B, 729, 1152] visual features
```

#### Usage

```python
from models import VisionEncoder

# Load from local cache (fast) or HuggingFace (slow)
encoder = VisionEncoder.from_pretrained()
encoder.freeze()  # Freeze for VLM training

# Forward pass
images = torch.randn(2, 3, 384, 384)
features = encoder(images)  # [2, 729, 1152]
```

---

### 2. Language Model (`language_model.py`)

**Model**: Qwen3-0.6B-Base  
**Parameters**: 596M (trained during VLM training)  
**Source**: [Qwen/Qwen3-0.6B-Base](https://huggingface.co/Qwen/Qwen3-0.6B-Base)

#### Architecture

| Component | Value |
|-----------|-------|
| Hidden Dim | 1024 |
| MLP Dim | 3072 |
| Layers | 28 |
| Q Heads | 16 |
| KV Heads | 8 (GQA) |
| Head Dim | 128 (explicit) |
| Vocab Size | 151,936 |
| Max Context | 32,768 |
| RoPE Î¸ | 1,000,000 |

#### Key Features (Qwen3-specific)

1. **Grouped Query Attention (GQA)**: 16 query heads share 8 key-value heads
2. **Explicit Head Dim**: 128 (larger than hidden_size/num_heads = 64)
3. **Q/K Normalization**: RMSNorm applied to Q and K before RoPE
4. **No Attention Bias**: All projection layers have `bias=False`

#### Data Flow

```
Input: [B, L] token IDs
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Token Embedding    â”‚  [B, L] â†’ [B, L, 1024]
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Position IDs       â”‚  Generate [0, 1, 2, ..., L-1]
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Decoder Layers (Ã—28)                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ Input Norm (RMSNorm)                                    â”‚â”‚
â”‚  â”‚     â†“                                                   â”‚â”‚
â”‚  â”‚ Self-Attention (GQA + SDPA)                             â”‚â”‚
â”‚  â”‚   â€¢ Q: 1024 â†’ 2048 (16 heads Ã— 128 dim)                â”‚â”‚
â”‚  â”‚   â€¢ K: 1024 â†’ 1024 (8 heads Ã— 128 dim)                 â”‚â”‚
â”‚  â”‚   â€¢ V: 1024 â†’ 1024 (8 heads Ã— 128 dim)                 â”‚â”‚
â”‚  â”‚   â€¢ Q/K Norm â†’ RoPE â†’ SDPA â†’ O Proj                    â”‚â”‚
â”‚  â”‚     â†“ (+ residual)                                      â”‚â”‚
â”‚  â”‚ Post-Attention Norm (RMSNorm)                           â”‚â”‚
â”‚  â”‚     â†“                                                   â”‚â”‚
â”‚  â”‚ MLP (SwiGLU)                                            â”‚â”‚
â”‚  â”‚   â€¢ gate_proj: 1024 â†’ 3072                             â”‚â”‚
â”‚  â”‚   â€¢ up_proj:   1024 â†’ 3072                             â”‚â”‚
â”‚  â”‚   â€¢ down_proj: 3072 â†’ 1024                             â”‚â”‚
â”‚  â”‚   â€¢ output = down(SiLU(gate) * up)                     â”‚â”‚
â”‚  â”‚     â†“ (+ residual)                                      â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Final Norm         â”‚  RMSNorm
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LM Head            â”‚  Linear(1024 â†’ 151936)
â”‚  (tied weights)     â”‚  Shares weights with embedding
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
Output: [B, L, 151936] logits
```

#### Usage

```python
from models import LanguageModel

# Load from local cache (fast) or HuggingFace (slow)
llm = LanguageModel.from_pretrained()

# Forward pass
input_ids = torch.randint(0, 1000, (2, 64))
logits = llm(input_ids)  # [2, 64, 151936]

# With KV cache for generation
logits, cache = llm(input_ids, use_cache=True)
next_logits, cache = llm(next_token, past_key_values=cache, use_cache=True)
```

---

### 3. Connector (`connector.py`)

**Purpose**: Project vision features to LLM embedding space  
**Parameters**: 1-4M (trained during VLM training)

#### Options

| Type | Parameters | Description |
|------|------------|-------------|
| `linear` | ~1M | Simple linear projection (fastest) |
| `mlp` | ~4M | 2-layer MLP with GELU (more capacity) |
| `resampler` | ~8M | Cross-attention based (reduces tokens) |

#### Data Flow (Linear)

```
Input: [B, 729, 1152] vision features
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Linear Projection  â”‚  Linear(1152 â†’ 1024)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
Output: [B, 729, 1024] visual tokens (ready for LLM)
```

#### Data Flow (MLP)

```
Input: [B, 729, 1152] vision features
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FC1               â”‚  Linear(1152 â†’ 2048)
â”‚  GELU              â”‚
â”‚  FC2               â”‚  Linear(2048 â†’ 1024)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
Output: [B, 729, 1024] visual tokens
```

#### Usage

```python
from models import MultimodalConnector, ConnectorConfig

# Linear connector
config = ConnectorConfig(
    vision_dim=1152,
    llm_dim=1024,
    connector_type="linear"
)
connector = MultimodalConnector(config)

vision_features = torch.randn(2, 729, 1152)
visual_tokens = connector(vision_features)  # [2, 729, 1024]
```

---

### 4. Complete Model (`nanomolmo2.py`)

**NanoMolmo2**: Combines all components into a complete VLM.

#### Training Mode

```python
from models import NanoMolmo2

model = NanoMolmo2.from_pretrained()

# Forward pass with loss computation
logits, loss = model(
    input_ids=tokens,        # [B, L] text tokens with <image> placeholders
    pixel_values=images,     # [B, 3, 384, 384] RGB images
    labels=labels,           # [B, L] target tokens (-100 for non-loss positions)
)

# Backward pass
loss.backward()
```

#### Inference Mode

```python
# Generation
generated_ids = model.generate(
    prompt_ids,              # Initial prompt tokens
    pixel_values=images,     # Image(s) to condition on
    max_new_tokens=512,
    temperature=0.7,
)
```

#### What Happens Inside

1. **Image Encoding** (frozen):
   ```
   images â†’ VisionEncoder â†’ [B, 729, 1152]
   ```

2. **Projection** (trained):
   ```
   [B, 729, 1152] â†’ Connector â†’ [B, 729, 1024]
   ```

3. **Token Embedding**:
   ```
   input_ids â†’ Embedding â†’ [B, L, 1024]
   ```

4. **Sequence Construction**:
   ```
   Replace <image> tokens with visual tokens
   Final: [B, L + 729 - 1, 1024]
   ```

5. **LLM Forward** (trained):
   ```
   combined_embeds â†’ LLM Decoder â†’ [B, L', 151936]
   ```

6. **Loss Computation**:
   ```
   CrossEntropy(logits, labels, ignore_index=-100)
   ```

---

## Weight Loading

All models support efficient weight loading:

```bash
# Download weights once (recommended)
python scripts/download_model_weights.py

# Creates:
#   checkpoints/siglip2_so400m_384.pt (1.6 GB)
#   checkpoints/qwen3_0.6b_base.pt (1.1 GB)
```

```python
# Load from local cache (fast, ~5-10s)
vision = VisionEncoder.from_pretrained()
llm = LanguageModel.from_pretrained()

# Or load from HuggingFace (slow, ~30s)
vision = VisionEncoder.from_pretrained(cache_dir=None)
llm = LanguageModel.from_pretrained(cache_dir=None)
```

---

## Training Configuration

### Frozen vs Trainable

| Component | Trainable | Parameters | Gradient Memory |
|-----------|-----------|------------|-----------------|
| Vision Encoder | âŒ | 413M | 0 |
| Connector | âœ… | ~1M | ~4 MB |
| Language Model | âœ… | 596M | ~2.4 GB |
| **Total** | - | **1,010M** | **~2.4 GB** |

### Precision (per Molmo2 tech report)

| Operation | Precision |
|-----------|-----------|
| Most computation | bfloat16 |
| LayerNorm | float32 |
| RoPE | float32 |

---

## File Structure

```
models/
â”œâ”€â”€ __init__.py           # Exports all components
â”œâ”€â”€ README.md             # This file
â”œâ”€â”€ vision_encoder.py     # SigLIP2 implementation
â”œâ”€â”€ language_model.py     # Qwen3-0.6B implementation
â”œâ”€â”€ connector.py          # Vision-to-LLM projection
â””â”€â”€ nanomolmo2.py         # Complete VLM
```

---

## References

- [Molmo2 Tech Report](https://www.datocms-assets.com/64837/1766008501-molmo2-tech-report.pdf) - Architecture and training details
- [SigLIP 2](https://huggingface.co/google/siglip2-so400m-patch14-384) - Vision encoder
- [Qwen3-0.6B-Base](https://huggingface.co/Qwen/Qwen3-0.6B-Base) - Language model
- [RoPE Paper](https://arxiv.org/abs/2104.09864) - Rotary Position Embeddings
