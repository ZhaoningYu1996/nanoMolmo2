# nanoMolmo2 Model Configuration
# Educational VLM with frozen vision encoder and Qwen3-0.6B LLM

model_name: "nanoMolmo2-0.6B"

# Vision Encoder (FROZEN during training)
vision_encoder:
  model_name: "openai/clip-vit-large-patch14-336"
  freeze: true  # Do NOT update during training
  image_size: 336
  patch_size: 14
  hidden_size: 1024
  num_layers: 24
  num_heads: 16
  parameters: 307M  # frozen, not counted in trainable params
  
  # Preprocessing
  mean: [0.48145466, 0.4578275, 0.40821073]
  std: [0.26862954, 0.26130258, 0.27577711]

# Multimodal Connector (TRAINABLE)
connector:
  type: "linear"  # Options: "linear", "mlp"
  input_dim: 1024  # vision encoder output
  output_dim: 896  # LLM hidden size
  
  # If type = "mlp"
  mlp_hidden_size: 2048
  mlp_layers: 2
  activation: "gelu"
  dropout: 0.0
  
  parameters: ~1M  # trainable

# Language Model (TRAINABLE)
language_model:
  model_name: "Qwen/Qwen2.5-0.5B"
  hidden_size: 896
  num_layers: 24
  num_heads: 14
  vocab_size: 151936
  max_position_embeddings: 32768
  parameters: 494M  # trainable
  
  # Special tokens
  special_tokens:
    - "<image>"
    - "<video>"
    - "<point>"
    - "<timestamp>"
    - "<pad>"
  
  # Training dtype
  torch_dtype: "bfloat16"

# Total Model
total_parameters:
  vision_encoder: 307M  # frozen
  connector: 1M  # trainable
  language_model: 494M  # trainable
  trainable_total: 495M
  total: 802M

# Memory Requirements (per GPU, A100 40GB)
memory_breakdown:
  vision_encoder: 1.2GB  # includes activations, no gradients
  connector: 50MB
  language_model: 15GB  # params + gradients + optimizer + activations
  batch_data: 4GB
  total_per_gpu: ~20GB
  recommended_gpu: "A100 40GB or A100 80GB"
